<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Designing the OpGraph &mdash; TensorWrapper 1.0.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=8d563738"></script>
        <script src="../../_static/doctools.js?v=888ff710"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Designing the Sparse Map Component" href="sparse_maps.html" />
    <link rel="prev" title="Designing the Expression Component" href="expression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/logo_candybar.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.0.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../background/index.html">TensorWrapper Background</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Developer Documentation</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Design of TensorWrapper</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="motivation.html">Motivating TensorWrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html">Overview of TensorWrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="shape.html">Tensor Shape Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="symmetry.html">Designing the Symmetry Component</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparsity.html">Tensor Sparsity Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="layout.html">Designing the Layout Component</a></li>
<li class="toctree-l3"><a class="reference internal" href="allocator.html">Designing the Allocator</a></li>
<li class="toctree-l3"><a class="reference internal" href="buffer.html">Designing the Buffer</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor_wrapper.html">Designing <code class="docutils literal notranslate"><span class="pre">TensorWrapper</span></code> Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="expression.html">Designing the Expression Component</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Designing the OpGraph</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-is-the-opgraph-component">What is the OpGraph Component?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#why-do-we-need-an-opgraph">Why do we need an OpGraph?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#opgraph-considerations">OpGraph Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tensor-networks">Tensor Networks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#opgraph-notation">OpGraph Notation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#opgraph-design">OpGraph Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="#opgraph-api">OpGraph API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="sparse_maps.html">Designing the Sparse Map Component</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse_maps/index.html">Sparse Maps Sublibrary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bibliography/bibliography.html">References</a></li>
<li class="toctree-l1"><a class="reference external" href="https://nwchemex.github.io/TensorWrapper/tensorwrapper_cxx_api/index.html">C++ API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TensorWrapper</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Developer Documentation</a></li>
          <li class="breadcrumb-item"><a href="index.html">Design of TensorWrapper</a></li>
      <li class="breadcrumb-item active">Designing the OpGraph</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/NWChemEx/TensorWrapper/edit/master/docs/source/developer/design/op_graph.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="designing-the-opgraph">
<span id="tw-designing-the-opgraph"></span><h1>Designing the OpGraph<a class="headerlink" href="#designing-the-opgraph" title="Link to this heading"></a></h1>
<section id="what-is-the-opgraph-component">
<h2>What is the OpGraph Component?<a class="headerlink" href="#what-is-the-opgraph-component" title="Link to this heading"></a></h2>
<p>Program control flow is usually thought of in terms of graphs (specifically
<a class="reference internal" href="../../background/terminology.html#term-dag"><span class="std std-ref">directed acyclic graph (DAG)</span></a>s). Each node of the DAG is a task and edges indicate data
dependencies/flow. Once the user has told TensorWrapper what they want to do,
TensorWrapper converts that request into a DAG. The DAG is then passed to the
backend which is then charged with traversing the DAG and executing the tasks
it represents. Since the DAG generated by TensorWrapper will be comprised of
tensor algebra operations, we term this DAG the “OpGraph”.</p>
</section>
<section id="why-do-we-need-an-opgraph">
<h2>Why do we need an OpGraph?<a class="headerlink" href="#why-do-we-need-an-opgraph" title="Link to this heading"></a></h2>
<p>Performing tensor algebra in a performant manner requires careful planning of
the order tasks are done in, how they are done, and where they are done. In
computer science we usually tackle this problem with DAGs. The OpGraph component
is needed so that we can have a DAG representation of the user’s request.
Thinking of the user’s inputs as a <a class="reference internal" href="../../background/terminology.html#term-dsl"><span class="std std-ref">domain specific language (DSL)</span></a> the OpGraph component is then
the <a class="reference internal" href="../../background/terminology.html#term-ast"><span class="std std-ref">abstract syntax tree (AST)</span></a> resulting from parsing the DSL (strictly speaking a
<a class="reference internal" href="../../background/terminology.html#term-cst"><span class="std std-ref">concrete syntax tree (CST)</span></a> actually results from parsing the DSL, which is then mapped to
an AST, but that intermediate step is immaterial here). The association of the
OpGraph component with an AST suggests another use: an intermediate
representation of the calculation.</p>
<section id="expression-layer-vs-opgraph">
<h3>Expression Layer vs. OpGraph<a class="headerlink" href="#expression-layer-vs-opgraph" title="Link to this heading"></a></h3>
<p>Perhaps what is less clear is why do we need the expression layer AND the
OpGraph component? This question is brought on by the fact that there is a fair
amount of redundancy in the classes. Ultimately, the answer is a separation of
concerns. The expression layer is purely meant to capture what the user wants to
do. The OpGraph component is supposed to represent what TensorWrapper wants the
backend to do. Generally speaking what the user wants to do will be expressed in
terms of much coarser operations than what TensorWrapper wants the backend to
do.</p>
<p>On a related note, another key difference between the Expression layer and the
OpGraph component is the the Expression layer is designed so that it is easy for
the user to express their intent, whereas the OpGraph is designed to be easy
for a backend to traverse. Thus the Expression layer is equation-based whereas
the OpGraph component is graph-based.</p>
</section>
</section>
<section id="opgraph-considerations">
<h2>OpGraph Considerations<a class="headerlink" href="#opgraph-considerations" title="Link to this heading"></a></h2>
<dl class="simple" id="og-data-flow">
<dt>data flow</dt><dd><p>The graph needs to be capable of distinguishing between inputs and
outputs. There conceivably could be cycles if loops are involved.</p>
</dd>
</dl>
<dl class="simple" id="og-multi-appearance">
<dt>tensors appear multiple times</dt><dd><p>It is not uncommon for the same tensor to appear multiple times in a graph.
For example many matrix decompositions are expressed in terms of a tensor
and its transpose or a common intermediate may be used in several equations.
The point is, we can not assume that all tensors in the graph are unique.</p>
</dd>
</dl>
<dl class="simple" id="og-multi-sources">
<dt>multiple sources</dt><dd><p>The graph may have multiple inputs. The most obvious examples of this are
binary operations, like addition, which map two input tensors to a single
output tensor.</p>
</dd>
</dl>
<dl class="simple" id="og-multi-sinks">
<dt>Multiple sinks</dt><dd><p>The graph may have multiple results. A somewhat common example of this is
an eigen solver which takes an input matrix, diagonalizes it, and returns
the eigenvectors and the eigenvalues.</p>
</dd>
</dl>
<dl class="simple" id="og-data-references">
<dt>data references</dt><dd><p>The <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> must either have handles to the actual data, or somehow be
associated with the actual data in order for backends to translate the AST
into results. But plainly, if we want the backend to add two tensors, we
better give the backend the two tensors.</p>
</dd>
</dl>
<dl class="simple" id="og-expression-compatibility">
<dt>expression compatibility</dt><dd><p>The <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> object will ultimately be filled in by the objects in the
expression component. Thus the interface of the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> class needs to
be designed so that it is compatible with the <code class="docutils literal notranslate"><span class="pre">Expression</span></code> component’s
implementation (see <a class="reference internal" href="expression.html#designing-the-expression-component"><span class="std std-ref">Designing the Expression Component</span></a>).</p>
<ul class="simple">
<li><p>As a corollary, this means that each <code class="docutils literal notranslate"><span class="pre">Expression</span></code> object must be
representable as part of the resulting <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple" id="og-extensible">
<dt>extensible</dt><dd><p>While we will propose an initial set of basic operations, this set is not
unique, nor will all backends agree that the operations are actually “basic”.
An excellent example is tensor contraction, which most backends will actually
further decompose into permutations and a gemm (the BLAS routine for general
matrix multiplication). If at a later time we decide that we would like
the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> to be able to express tensor contraction as permutations and
a gemm, instead of a single operation, we will need to be able to add the
ability to denote a gemm in the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code>.</p>
</dd>
</dl>
<dl class="simple" id="og-backend-specific-basic-ops">
<dt>backend specific basic operations</dt><dd><p>Somewhat related to <a class="reference internal" href="#og-extensible"><span class="std std-ref">extensible</span></a> not every backend will expose
interfaces for every basic operation TensorWrapper knows about. We will need
a mechanism of somehow knowing which basic operations a backend supports and
a mechanism for rewriting an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> in terms of different operations.</p>
</dd>
</dl>
<section id="out-of-scope">
<h3>Out of Scope<a class="headerlink" href="#out-of-scope" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>Identifying unique/repeat tensors</dt><dd><p>The OpGraph component works with what it is given. If the OpGraph component
is told that there are two tensors in the graph, the OpGraph component is
going to assume there are two different tensors in the graph, and not a
single tensor used twice. Identifying common intermediates should happen
before forming the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> (or should be done as a transformation of
the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code>).</p>
</dd>
<dt>Optimizing the OpGraph</dt><dd><p>Along the lines of identifying unique/repeat tensors, the OpGraph layer will
not be responsible for optimizing the computations an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> object
represents. Optimizing the computations can be done by a layer which takes
an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> object, analyzes it, and creates a new optimized <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code>
object.</p>
</dd>
</dl>
</section>
</section>
<section id="tensor-networks">
<h2>Tensor Networks<a class="headerlink" href="#tensor-networks" title="Link to this heading"></a></h2>
<p>The idea that tensor algebra can be expressed as a graph is not something new
that we just stumbled on to. Mathematicians have known this for years. The
graphical representation of tensor algebra is usually termed a “tensor network”.
Here’s the basics (see <a class="reference internal" href="#og-references"><span class="std std-ref">References</span></a> for background/where I took this
from):</p>
<ul class="simple">
<li><p>Nodes of a graph are tensors, edges are contractions.</p></li>
<li><p>The shape of the node can be used to convey properties of the tensor.</p>
<ul>
<li><p>There seems to be several conventions in the wild.</p></li>
<li><p>Squares or circles are usually the default symbol and are drawn with the
same area (<em>vide infra</em>).</p></li>
<li><p>Use of the default symbol indicates that no additional properties for the
tensor are known/specified.</p></li>
<li><p>Tensors which result from combining other tensors are denoted with
rectangles or ovals whose area reflects the total number of “default”
tensors which were consumed.</p></li>
</ul>
</li>
<li><p>Edges denote tensor modes.</p></li>
<li><p>The number of edges connected to a node equals the rank of that tensor.</p></li>
<li><p>Reshaping a tensor (typically flattening it) is done by combing edges into a
single edge.</p>
<ul>
<li><p>The thickness of the resulting edge is proportional to the number of
modes comprising it. For example, reshaping a matrix into a vector would
result in an edge which is twice as thick as the edges which were combined.</p></li>
</ul>
</li>
<li><p>Edges can be labeled with dummy indices to make referring to a particular
edge easier.</p></li>
<li><p>Contraction over multiple indices is denoted with parallel edges.</p></li>
<li><p>The trace of a tensor (or product of tensors) is denoted with a loop.</p></li>
<li><p>Additional operations, such as tensor products, addition, element-wise
addition, etc. are usually specified with rectangles/ovals labeled
with the operation.</p>
<ul>
<li><p>The area rules specified above apply to the resulting node.</p></li>
</ul>
</li>
<li><p>Tensor decompositions are usually represented by left and right pointing
triangles (the left pointing triangle being the adjoint of the right pointing
triangle). If the decomposition also results in “values”, <em>e.g.</em>, eigenvalues
or the singular values from a singular value decomposition, those values are
represented by a square/circle between the triangles.</p></li>
</ul>
<section id="why-not-use-tensor-networks-for-the-opgraph">
<h3>Why not use tensor networks for the OpGraph?<a class="headerlink" href="#why-not-use-tensor-networks-for-the-opgraph" title="Link to this heading"></a></h3>
<p>Tensor networks really seem to be geared at expressing contractions involving
a number of tensors. While this is a very important use case for the OpGraph
component, consideration <a class="reference internal" href="#og-expression-compatibility"><span class="std std-ref">expression compatibility</span></a> requires that our
graph also be capable of expressing other operations too. While tensor networks
have a mechanism for this (recursive or hierarchical nodes, <em>i.e.</em>, nodes which
actually
represent entire tensor networks themselves) this representation makes it hard
to address consideration <a class="reference internal" href="#og-multi-appearance"><span class="std std-ref">tensors appear multiple times</span></a>. In particular, if the same
tensor is involved in a contraction and say an addition, then it is difficult
to express that it is indeed the same tensor (which in graph notation is
naturally done by having it be literally the same node).</p>
<p>Another large problem with directly using a tensor network is tracking
permutations. Tensor networks treat modes of a tensor as if only the number of
modes (and the number of those modes which are contracted) matters. In practice,
having to permute modes can have huge performance consequences and it must be
considered (this is part of consideration <a class="reference internal" href="#og-expression-compatibility"><span class="std std-ref">expression compatibility</span></a>).
In theory this could be worked into the tensor network by enforcing an order to
the edges (<em>e.g.</em>, by requiring the edge for mode 0 to be the left most edge,
the edge for mode 1 to be the second left most edge, etc.). Then permutations
would manifest as edge crossings.</p>
<p>Ultimately, tensor networks were not designed to be task graphs, which is really
what the OpGraph component is after. Tensor networks are useful for expressing
the part of the task graph which maps to a specific tensor contraction, but
beyond that they are cumbersome to manipulate when multiple terms are equations
are involved. For this reason we have opted to generalize tensor networks.</p>
</section>
</section>
<section id="opgraph-notation">
<h2>OpGraph Notation<a class="headerlink" href="#opgraph-notation" title="Link to this heading"></a></h2>
<p>The graph represented by an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> object can be considered a
generalization of a tensor network, with extensions to accommodate the extended
set of use cases the OpGraph component must deal with. The need for being able
to visualize an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> object graphically is useful for design, and is
expected to also be useful for code analysis/optimization. To that end we
propose the following notation:</p>
<ul class="simple">
<li><p>Nodes of the graph depict either tensors or operators.</p>
<ul>
<li><p>Tensors are denoted with square nodes.</p></li>
<li><p>Operations with circles.</p></li>
<li><p>Nodes will be labeled with the name of the tensor or the operation.</p></li>
<li><p>Unlike traditional tensor networks, most operations are treated the same.
The key exception is permutations which are carried on the edges instead
of the nodes.</p></li>
</ul>
</li>
<li><p>Edges denote modes.</p>
<ul>
<li><p>Parallel edges are avoided by fusing indices, <em>i.e.</em>, each edge is labeled
with all indices participating in that operation.</p></li>
<li><p>The number of fused modes is tracked by annotating the modes.</p></li>
<li><p>The annotations are used to express permutations (and for the multiplication
operation convey generalized Einstein summation convention).</p></li>
</ul>
</li>
<li><p>Edges are directed.</p>
<ul>
<li><p>The direction indicates data flow. Sources are input tensors. Sinks are
outputs.</p></li>
<li><p>The rank of a tensor can be determined from the number of unique
indices associated with it.</p></li>
</ul>
</li>
</ul>
<section id="opgraph-structure">
<h3>OpGraph Structure<a class="headerlink" href="#opgraph-structure" title="Link to this heading"></a></h3>
<figure class="align-center" id="id1">
<span id="fig-narity"></span><img alt="../../_images/narity.png" src="../../_images/narity.png" />
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Overview of how operations of different <a class="reference internal" href="../../background/terminology.html#term-arity"><span class="std std-ref">arity</span></a> look using OpGraph
graphical notation. For simplicity, mode annotations and operation labels are
not specified.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-narity"><span class="std std-ref">Overview of how operations of different term_arity look using OpGraph
graphical notation. For simplicity, mode annotations and operation labels are
not specified.</span></a> illustrates how an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> representation looks for
operations of various <a class="reference internal" href="../../background/terminology.html#term-arity"><span class="std std-ref">arity</span></a>. Graphs are grouped into a matrix such
that for row <span class="math notranslate nohighlight">\(m\)</span> (<span class="math notranslate nohighlight">\(m\)</span> is 1-based) the <span class="math notranslate nohighlight">\(n\)</span>-th column (<span class="math notranslate nohighlight">\(n\)</span> is 0-based) denotes an
operation returning <span class="math notranslate nohighlight">\(m\)</span> tensors given <span class="math notranslate nohighlight">\(n\)</span> tensors (<span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span> have different
bases because operations returning no tensors are not interesting).</p>
<p>The simplest, non-null, <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> stems from simply declaring a tensor. The
resulting “nullary” graph for a tensor <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, is shown in <a class="reference internal" href="#fig-narity"><span class="std std-ref">Overview of how operations of different term_arity look using OpGraph
graphical notation. For simplicity, mode annotations and operation labels are
not specified.</span></a>. From
the perspective of the OpGraph component, the actual declaration of a tensor
requires performing some opaque operation (such operations are denoted by purple
circles in <a class="reference internal" href="#fig-narity"><span class="std std-ref">Overview of how operations of different term_arity look using OpGraph
graphical notation. For simplicity, mode annotations and operation labels are
not specified.</span></a>). For declaring a tensor this operation simply
returns the tensor (and does not require any input to do so, hence it is a
nullary operation). From the perspective of <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code>, the nullary operations
which create the source tensors must always be present and they are not usually
interesting (effectively being lambdas like <cite>[](){return A;}</cite>). Thus by
convention, and in an effort to simplify the representation of <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code>
objects, the nullary operations giving rise to the source tensors will usually
be implicit. The exception being when those nullary operations are interesting
(usually because they are on-demand generator functions). For the remaining
columns in <a class="reference internal" href="#fig-narity"><span class="std std-ref">Overview of how operations of different term_arity look using OpGraph
graphical notation. For simplicity, mode annotations and operation labels are
not specified.</span></a> this convention applies. As shown in row 2 of
<a class="reference internal" href="#fig-narity"><span class="std std-ref">Overview of how operations of different term_arity look using OpGraph
graphical notation. For simplicity, mode annotations and operation labels are
not specified.</span></a></p>
<p>The next simplest <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> requires mapping an input tensor to an output
tensor via some intermediate operation. Such operations are “unary” and examples
include permuting the modes of a tensor, scaling a tensor, and raising a tensor
to a power. It is also possible that a unary operation returns multiple tensors,
<em>e.g.</em>, a standard eigen solver which returns the eigenvectors and the
eigenvalues. At this point, the basic structure of an operation should be clear,
nonetheless <a class="reference internal" href="#fig-narity"><span class="std std-ref">Overview of how operations of different term_arity look using OpGraph
graphical notation. For simplicity, mode annotations and operation labels are
not specified.</span></a> shows examples of some other arities.</p>
</section>
<section id="basic-operations">
<h3>Basic Operations<a class="headerlink" href="#basic-operations" title="Link to this heading"></a></h3>
<figure class="align-center" id="id2">
<span id="fig-basic-operations"></span><img alt="../../_images/opgraph_basic_ops.png" src="../../_images/opgraph_basic_ops.png" />
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Pictorial representations of the fundamental operations of the OpGraph
component.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-basic-operations"><span class="std std-ref">Pictorial representations of the fundamental operations of the OpGraph
component.</span></a> shows some of the basic operations which will be
comprise actual <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> instances. For simplicity we have focused on matrix
operations (most input/output edges have two annotations), but much of what is
in <a class="reference internal" href="#fig-basic-operations"><span class="std std-ref">Pictorial representations of the fundamental operations of the OpGraph
component.</span></a> generalizes to other rank tensors in a
straightforward manner. Ignoring nullary operations, all operation nodes have
one or more inputs and one or more outputs. The goal is to establish a small set
of “fundamental” operations and to write all other operations in terms of these
operations. For example, we do not define a chip operation, but a chip operation
can be defined by a slice followed by a reshape.</p>
<p>As shown in <a class="reference internal" href="#fig-basic-operations"><span class="std std-ref">Pictorial representations of the fundamental operations of the OpGraph
component.</span></a>, tensors acting as inputs to an
operation have their annotations associated with the edge connecting them to
the operation. Tensors resulting from an operation have their annotations
associated with the edge connecting the the operation to the tensor. In turn
permutations are signified by reordering the output indices relative to the
input indices.</p>
<p>The most questionable choice we have made is the “multiplication” operator. The
multiplication operator actually stands in for a number of operations including
trace, contraction, tensor product, and element-wise multiplication (though we
have also defined an element-wise multiplication operator for consistency with
the other element-wise operations). Our motivation here is that many of the
backend tensor libraries have already invested in infrastructure for handling
generalized Einstein summation convention (and/or tensor networks) and in the
first pass we intend to dispatch to the backend’s implementations.</p>
</section>
<section id="more-complicated-opgraphs">
<h3>More Complicated OpGraphs<a class="headerlink" href="#more-complicated-opgraphs" title="Link to this heading"></a></h3>
<p>Ultimately, the state of an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> is obtained by combining basic
operations from the previous subsection into larger graphs. From the
<a class="reference internal" href="expression.html#designing-the-expression-component"><span class="std std-ref">Designing the Expression Component</span></a> section the first our more complicated
code examples was:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="k">auto</span><span class="w"> </span><span class="n">aijk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">(</span><span class="s">&quot;i,j,k&quot;</span><span class="p">);</span>
<span class="w">   </span><span class="n">c</span><span class="p">(</span><span class="s">&quot;i,j,k&quot;</span><span class="p">)</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">aijk</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">(</span><span class="s">&quot;i,j,k&quot;</span><span class="p">);</span>
<span class="w">   </span><span class="n">d</span><span class="p">(</span><span class="s">&quot;i,j,k&quot;</span><span class="p">)</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">aijk</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">b</span><span class="p">(</span><span class="s">&quot;i,j,k&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<figure class="align-center" id="id3">
<span id="fig-complicated-graphs"></span><img alt="../../_images/complicated_graphs.png" src="../../_images/complicated_graphs.png" />
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Graphs resulting from the “complicated” code snippets.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The graph resulting from this code is shown on the left of
<a class="reference internal" href="#fig-complicated-graphs"><span class="std std-ref">Graphs resulting from the “complicated” code snippets.</span></a>. The graph expresses that after creation <code class="docutils literal notranslate"><span class="pre">a</span></code> is
used in two equations, which in turn result in two outputs. As was discussed
in <a class="reference internal" href="expression.html#designing-the-expression-component"><span class="std std-ref">Designing the Expression Component</span></a> TensorWrapper will not be able to
identify common intermediates at first, so it treats <code class="docutils literal notranslate"><span class="pre">b</span></code> as two separate
tensors. The next most complicated code example we showed in
<a class="reference internal" href="expression.html#designing-the-expression-component"><span class="std std-ref">Designing the Expression Component</span></a> was:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="n">L</span><span class="p">,</span><span class="w"> </span><span class="n">Lt</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">λ</span><span class="p">,</span><span class="w"> </span><span class="n">a10_10</span><span class="p">,</span><span class="w"> </span><span class="n">a2</span><span class="p">;</span>
<span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="k">auto</span><span class="w"> </span><span class="n">Aij</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// disclaimer, I&#39;m not 100% sure the cholesky/eigen_solve APIs will work</span>
<span class="w">    </span><span class="c1">// as shown, but it should be possible to get something close.</span>

<span class="w">    </span><span class="c1">// A = LLt</span>
<span class="w">    </span><span class="n">L</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cholesky</span><span class="p">(</span><span class="n">Aij</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Av = λBv (no argument needed if B is 1)</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">v</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">),</span><span class="w"> </span><span class="n">λ</span><span class="p">(</span><span class="s">&quot;j&quot;</span><span class="p">)]</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">eigen_solve</span><span class="p">(</span><span class="n">Aij</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">));</span>

<span class="w">    </span><span class="c1">// Get the  slice of A starting a 0,0 and extending to 10,10 exclusive.</span>
<span class="w">    </span><span class="n">a10_10</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">slice</span><span class="p">(</span><span class="n">Aij</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">});</span>

<span class="w">    </span><span class="c1">// Raise A to the power 2</span>
<span class="w">    </span><span class="n">a2</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pow</span><span class="p">(</span><span class="n">Aij</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The graph resulting from this code snippet is shown on the right of
<a class="reference internal" href="#fig-complicated-graphs"><span class="std std-ref">Graphs resulting from the “complicated” code snippets.</span></a>. Here the intermediate <code class="docutils literal notranslate"><span class="pre">A</span></code> is used in four
different expressions including some expressions with multiple return values.</p>
</section>
</section>
<section id="opgraph-design">
<h2>OpGraph Design<a class="headerlink" href="#opgraph-design" title="Link to this heading"></a></h2>
<figure class="align-center" id="id4">
<span id="fig-opgraph-classes"></span><img alt="../../_images/opgraph.png" src="../../_images/opgraph.png" />
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">The classes comprising</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>So far we have focused exclusively on the graph representation of the OpGraph
and not the programmatic design. <a class="reference internal" href="#fig-opgraph-classes"><span class="std std-ref">The classes comprising</span></a> shows the classes
comprising the OpGraph component of TensorWrapper. The classes are summarized
in more detail in the following subsections</p>
<section id="opgraph">
<h3><code class="docutils literal notranslate"><span class="pre">OpGraph</span></code><a class="headerlink" href="#opgraph" title="Link to this heading"></a></h3>
<p>This is a container-like class which stores the actual DAG. The elements of the
<code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> class are <code class="docutils literal notranslate"><span class="pre">Edge</span></code> and <code class="docutils literal notranslate"><span class="pre">Node</span></code> objects. The <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> object
additionally knows the connectivity of the graph and properties of the graph
(<em>e.g.</em>, the number of sinks or sources).</p>
</section>
<section id="edge-node">
<h3><code class="docutils literal notranslate"><span class="pre">Edge</span></code> / <code class="docutils literal notranslate"><span class="pre">Node</span></code><a class="headerlink" href="#edge-node" title="Link to this heading"></a></h3>
<p>The fundamental elements of the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> class are <code class="docutils literal notranslate"><span class="pre">Edge</span></code> and <code class="docutils literal notranslate"><span class="pre">Node</span></code>
objects. <code class="docutils literal notranslate"><span class="pre">Edge</span></code> objects represent tensor modes and the directionality of the
edge indicates whether a tensor is an input or an output. <code class="docutils literal notranslate"><span class="pre">Node</span></code> objects
represent either a tensor or an operation with tensors only being connected to
operations and operations only being connected to tensors (<em>i.e.</em>, every
<code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> is a bipartite graph).</p>
</section>
<section id="basicops">
<h3><code class="docutils literal notranslate"><span class="pre">BasicOps</span></code><a class="headerlink" href="#basicops" title="Link to this heading"></a></h3>
<p>Pursuant to consideration <a class="reference internal" href="#og-backend-specific-basic-ops"><span class="std std-ref">backend specific basic operations</span></a> the OpGraph
component needs a way to be able to know what basic operations a backend can
handle. To this end we introduce the <code class="docutils literal notranslate"><span class="pre">BasicOps</span></code> class. The <code class="docutils literal notranslate"><span class="pre">BasicOps</span></code> class
is envisioned as being more or less a strong type over a <code class="docutils literal notranslate"><span class="pre">std::set&lt;Node&gt;</span></code>.
For each backend, TensorWrapper would maintain a <code class="docutils literal notranslate"><span class="pre">BasicOps</span></code> filled with the
operations that backend can parse. Before calling the backend with a specific
<code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> TensorWrapper will ensue that the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> is comprised entirely
of operations the backend understands. If it is not, TensorWrapper will either
rewrite the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> in terms of operations the backend can understand or
error out.</p>
</section>
<section id="operations">
<h3>Operations<a class="headerlink" href="#operations" title="Link to this heading"></a></h3>
<p>The remaining classes in <a class="reference internal" href="#fig-opgraph-classes"><span class="std std-ref">The classes comprising</span></a> represent the basic
operations TensorWrapper knows about. Many of these classes are simply strong
types, although some, like <code class="docutils literal notranslate"><span class="pre">Scale</span></code>, will also contain state. By having the
various operations each have their own class we can address
<a class="reference internal" href="#og-data-references"><span class="std std-ref">data references</span></a>. New operations can be added, thus satisfying the
<a class="reference internal" href="#og-extensible"><span class="std std-ref">extensible</span></a> consideration, by
deriving new classes. The operations will only affect backends whose
corresponding <code class="docutils literal notranslate"><span class="pre">BasicOps</span></code> object is updated (thus preserving backwards
compatibility).</p>
<p>In satisfying <a class="reference internal" href="#og-expression-compatibility"><span class="std std-ref">expression compatibility</span></a> we note that many classes in
the Expression component have an analogous class in the OpGraph component. For
those that don’t we explicitly note the mapping from the Expression component
to the OpGraph component in the following list:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Indexed</span></code> essentially maps to a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> node plus an <code class="docutils literal notranslate"><span class="pre">Edge</span></code> object.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Chip</span></code> is <code class="docutils literal notranslate"><span class="pre">Slice</span></code> followed by a reshape.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Permutation</span></code> is determined by comparing the input and output <code class="docutils literal notranslate"><span class="pre">Edge</span></code>
objects.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">EigenVectors</span></code> and <code class="docutils literal notranslate"><span class="pre">EigenValues</span></code> become <code class="docutils literal notranslate"><span class="pre">EigenSolve</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AssignTo</span></code> maps to an edge stemming from a operation <code class="docutils literal notranslate"><span class="pre">Node</span></code>.</p></li>
</ul>
</section>
</section>
<section id="opgraph-api">
<h2>OpGraph API<a class="headerlink" href="#opgraph-api" title="Link to this heading"></a></h2>
<p>Before discussing the API of the OpGraph component we want to remind the reader
that OpGraph objects will in general be generated by the expression layer. Hence
we fully expect users to interact with TensorWrapper through the expression
layer and will rely on the expression layer to generate <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> objects. In
turn, while the following code snippets are verbose we feel that is okay because
users will not be writing them.</p>
<p>The API of the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> component is modeled after the Boost Graph Library
(see <a class="reference external" href="https://www.boost.org/doc/libs/1_83_0/libs/graph/doc/">here</a>). This
is to lower the barrier to entry in case the user is already familiar with that
library and so that an actual graph library (like Boost Graph Library) can be
wrapped by <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> if needed for performance.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> class serves the role of an overall container for the graph. A
similar role to say <code class="docutils literal notranslate"><span class="pre">boost::adjacency_matrix</span></code> or <code class="docutils literal notranslate"><span class="pre">boost::adjacency_list</span></code>
classes.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">opgraph</span><span class="p">;</span><span class="w"> </span><span class="c1">// OpGraph component lives in opgraph namespace</span>

<span class="c1">// Will be graph for A + B = C</span>
<span class="n">OpGraph</span><span class="w"> </span><span class="n">g</span><span class="p">;</span><span class="w">     </span><span class="c1">// Default graph, no nodes</span>

<span class="c1">// Will be graph for pow(A, 2) = C</span>
<span class="n">OpGraph</span><span class="w"> </span><span class="nf">g3</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span><span class="w"> </span><span class="c1">// Graph which will pre-allocate room for 3 nodes.</span>

<span class="n">TensorWrapper</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">;</span><span class="w"> </span><span class="c1">// Assume these are set up already</span>

<span class="c1">// Creates three nodes which respectively represent tensor A, B, and C</span>
<span class="n">g</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">Tensor</span><span class="p">{</span><span class="n">A</span><span class="p">});</span><span class="w"> </span><span class="c1">// Will be node 0,</span>
<span class="n">g</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">Tensor</span><span class="p">{</span><span class="n">B</span><span class="p">});</span><span class="w"> </span><span class="c1">// node 1,</span>
<span class="n">g</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">Add</span><span class="p">{});</span><span class="w">     </span><span class="c1">// node 2,</span>
<span class="n">g</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">Tensor</span><span class="p">{</span><span class="n">C</span><span class="p">});</span><span class="w"> </span><span class="c1">// and node 3</span>

<span class="n">g3</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">Tensor</span><span class="p">{</span><span class="n">A</span><span class="p">});</span>
<span class="n">g3</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">Pow</span><span class="p">{</span><span class="mi">2</span><span class="p">});</span><span class="w">  </span><span class="c1">// Operation which squares a matrix</span>
<span class="n">g3</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">C</span><span class="p">));</span>

<span class="n">g</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;i,j&quot;</span><span class="p">);</span><span class="w"> </span><span class="c1">// Adds an edge from node 0 to node 2 labeled &quot;i,j&quot;</span>
<span class="n">g</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;i,j&quot;</span><span class="p">);</span><span class="w"> </span><span class="c1">// Similar, but edge goes from 1 to 2</span>
<span class="n">g</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;i,j&quot;</span><span class="p">);</span><span class="w"> </span><span class="c1">// Similar, but edge goes from 2 to 3</span>

<span class="n">g3</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;i,j&quot;</span><span class="p">);</span>
<span class="n">g3</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;i,j&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>Since Nodes can in general have the same value we can’t use the value of a node
as a “key” and we must refer to nodes by offset. For example, while an API like
<code class="docutils literal notranslate"><span class="pre">g.add_edge(Tensor{A},</span> <span class="pre">Add{},</span> <span class="pre">&quot;i,j&quot;);</span></code> would be user-friendly, if there were
say two add operations we wouldn’t know which <code class="docutils literal notranslate"><span class="pre">Add</span></code> instance to connect to
<code class="docutils literal notranslate"><span class="pre">Tensor{A}</span></code>. While it’s tempting to say that all <code class="docutils literal notranslate"><span class="pre">Add</span></code> instances are the
same, and thus we should just be connecting all addition operations to the same
node, doing so sacrifices the data dependency order (assume we only have a
single <code class="docutils literal notranslate"><span class="pre">Add</span></code> node, then if we add the output from an <code class="docutils literal notranslate"><span class="pre">Add</span></code> operation to
another tensor the resulting graph has a loop).</p>
<p>After we have created an <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> we can inspect it:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume it&#39;s the same g from above</span>

<span class="c1">// Returns a pair of iterators over the nodes in the graph</span>
<span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">node_begin</span><span class="p">,</span><span class="w"> </span><span class="n">node_end</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">nodes</span><span class="p">();</span>

<span class="c1">// Returns a pair of iterators over the edges in the graph</span>
<span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">edge_being</span><span class="p">,</span><span class="w"> </span><span class="n">edge_end</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">edges</span><span class="p">();</span>

<span class="c1">// Number of nodes/edges</span>
<span class="k">auto</span><span class="w"> </span><span class="n">nnodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">num_nodes</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">nedges</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">num_edges</span><span class="p">();</span>

<span class="c1">// The degree of a node is the total number of edges connected to it</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">degree</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">degree</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">degree</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">degree</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>

<span class="c1">//The in degree of a node is the number of nodes which feed into it</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">in_degree</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">in_degree</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">in_degree</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">in_degree</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>

<span class="c1">// The out degree is the number of nodes a node feeds in to</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">out_degree</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">out_degree</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">out_degree</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">out_degree</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>

<span class="c1">// Returns a pair of iterators over the edges connected to the specified</span>
<span class="c1">// node. Here node 0</span>
<span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">edges0_begin</span><span class="p">,</span><span class="w"> </span><span class="n">edges0_end</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">edges</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">((</span><span class="o">*</span><span class="n">edges0_begin</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">Edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;i,j&quot;</span><span class="p">));</span><span class="w"> </span><span class="c1">// One edge going from 0 to 1</span>

<span class="c1">// Only the edges going in to node 0</span>
<span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">in_edges0_begin</span><span class="p">,</span><span class="w"> </span><span class="n">in_edges0_end</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">in_edges</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">in_edges0_begin</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">in_edges0_end</span><span class="p">);</span><span class="w"> </span><span class="c1">// There are none</span>

<span class="c1">// Only the edges going out of node 0</span>
<span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">out_edges0_begin</span><span class="p">,</span><span class="w"> </span><span class="n">out_edges0_end</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">out_edges</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">((</span><span class="o">*</span><span class="n">out_edges0_begin</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">Edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;i,j&quot;</span><span class="p">));</span>

<span class="c1">// Returns a pair of iterators over the nodes connected to the specified</span>
<span class="c1">// node, here node 0</span>
<span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">nodes0_begin</span><span class="p">,</span><span class="w"> </span><span class="n">nodes0_end</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="p">.</span><span class="n">adjacent_nodes</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">((</span><span class="o">*</span><span class="n">nodes0_begin</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>

<span class="c1">// OpGraph is a DAG so a pair of numbers maps to exactly one edge, *i.e.* :</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">source</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">source</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">sink</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">sink</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
<p>While not shown, we anticipate that a series of free functions will be needed
for computing properties of <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> objects or running say depth-first
searches on them. We anticipate the such functions will wrap existing algorithms
supplied by the backend of the <code class="docutils literal notranslate"><span class="pre">OpGraph</span></code> component.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p>The design of the OpGraph component satisfies the above considerations by:</p>
<dl class="simple">
<dt><a class="reference internal" href="#og-data-flow"><span class="std std-ref">data flow</span></a></dt><dd><p>Edges are directed and indicate whether the tensor connected to the edge
is going into the operation or coming from it.</p>
</dd>
<dt><a class="reference internal" href="#og-multi-appearance"><span class="std std-ref">tensors appear multiple times</span></a></dt><dd><p>The same <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> object can be reused when an intermediate appears
multiple times. Alternatively, different <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> objects can be created
which point to the same intermediate.</p>
</dd>
<dt><a class="reference internal" href="#og-multi-sources"><span class="std std-ref">multiple sources</span></a></dt><dd><p>Operations may have inputs which come from different tensors.</p>
</dd>
<dt><a class="reference internal" href="#og-multi-sinks"><span class="std std-ref">Multiple sinks</span></a></dt><dd><p>Operations may point to (<em>i.e.</em> return) more than tensor.</p>
</dd>
<dt><a class="reference internal" href="#og-data-references"><span class="std std-ref">data references</span></a></dt><dd><p>The nodes of the graph are ultimately classes. Each operation is its own
class and thus can store additional state if need be.</p>
</dd>
<dt><a class="reference internal" href="#og-expression-compatibility"><span class="std std-ref">expression compatibility</span></a></dt><dd><p>The initial design of the OpGraph component includes operations for most of
the classes defined in the Expression component. For the remaining Expression
component objects simple straightforward mappings to two or more OpGraph
components exist.</p>
</dd>
<dt><a class="reference internal" href="#og-extensible"><span class="std std-ref">extensible</span></a></dt><dd><p>Additional operations can be added by deriving from the <code class="docutils literal notranslate"><span class="pre">Node</span></code> class.</p>
</dd>
<dt><a class="reference internal" href="#og-backend-specific-basic-ops"><span class="std std-ref">backend specific basic operations</span></a></dt><dd><p>Each backend will be associated with a <code class="docutils literal notranslate"><span class="pre">BasicOps</span></code> object which details the
basic operations the backend can parse. Maintainers of TensorWrapper will be
responsible for maintaining the <code class="docutils literal notranslate"><span class="pre">BasicOps</span></code> object.</p>
</dd>
</dl>
</section>
<section id="references">
<span id="og-references"></span><h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p>For the tensor network background we primarily relied on sources found in the
README of Google’s
<a class="reference external" href="https://github.com/google/TensorNetwork#readme">TensorNetwork</a> project,
specifically:</p>
<ul class="simple">
<li><p><cite>https://iopscience.iop.org/article/10.1088/1751-8121/aa6dc3/pdf</cite></p></li>
<li><p><cite>https://arxiv.org/pdf/1306.2164.pdf</cite></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="expression.html" class="btn btn-neutral float-left" title="Designing the Expression Component" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sparse_maps.html" class="btn btn-neutral float-right" title="Designing the Sparse Map Component" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, NWChemEx Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>