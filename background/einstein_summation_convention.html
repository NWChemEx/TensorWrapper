<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Einstein Summation Convention &mdash; TensorWrapper 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=8d563738"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Logical Versus Physical: Understanding the Tensor Layout" href="logical_v_physical.html" />
    <link rel="prev" title="TensorWrapper Terminology" href="terminology.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/logo_candybar.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.0.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">TensorWrapper Background</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="terminology.html">TensorWrapper Terminology</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Einstein Summation Convention</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#motivation">Motivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="logical_v_physical.html">Logical Versus Physical: Understanding the Tensor Layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="nested_tensors.html">Understanding Nested Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="key_features.html">Key Features of TensorWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="other_choices.html">Existing Tensor Libraries</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../developer/index.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_maps/index.html">Sparse Maps Sublibrary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography/bibliography.html">References</a></li>
<li class="toctree-l1"><a class="reference external" href="https://nwchemex.github.io/TensorWrapper/tensorwrapper_cxx_api/index.html">C++ API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TensorWrapper</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">TensorWrapper Background</a></li>
      <li class="breadcrumb-item active">Einstein Summation Convention</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/NWChemEx/TensorWrapper/edit/master/docs/source/background/einstein_summation_convention.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="einstein-summation-convention">
<span id="id1"></span><h1>Einstein Summation Convention<a class="headerlink" href="#einstein-summation-convention" title="Link to this heading"></a></h1>
<p>The purpose of this page is to give a primer on the Einstein summation
conventions and to point out why they are useful and when they can’t be used.</p>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Link to this heading"></a></h2>
<p>Many tensor operations involve summing over pairs of repeated indices. For
example, the inner-product between two vectors, <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, looks like:</p>
<div class="math notranslate nohighlight">
\[c = \sum_{i} u_i v_i.\]</div>
<p>Other prominent examples include the product of a vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> with a matrix <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{vM}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\left[\mathbf{vM}\right]_j = \sum_{i} v_i M_{ij}\]</div>
<p>and the matrix-matrix product, <span class="math notranslate nohighlight">\(\mathbf{C}\)</span>, between two matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>:</p>
<div class="math notranslate nohighlight">
\[C_{ij} = \sum_{k} A_{ik}B_{kj}.\]</div>
<p>Given the prevalence of such tensor operations people wanting to save some
writing/typing often forgo the explicit summation symbols and agree on a
summation convention which says that pairs of repeated indices appearing in a
term are summed over. This summation convention was brought to physics by Albert
Einstein in his work on general relativity and is thus commonly known as the
Einstein summation convention. Using the Einstein summation convention the
above three equations would look like:</p>
<div class="math notranslate nohighlight">
\[\begin{split}c =&amp; u_i v_i\\
\left[\mathbf{vM}\right]_j =&amp; \sum_{i} v_i M_{ij}\\
C_{ij} =&amp; \sum_{k} A_{ik}B_{kj}\end{split}\]</div>
<p>The traditional Einstein summation convention doesn’t allow for element-wise
products like (note there is no implicit summation in the next equation):</p>
<div class="math notranslate nohighlight">
\[C_{ij} = A_{ij}B_{ij}.\]</div>
<p>Making the observation that indices which are summed over only appear on one
side of the equation. We can define a “generalized Einstein summation
convention” which says that if an index only appears on a single side of an
equation it is implicitly summed over. Note that this also relaxes the “pair”
restriction so summing over a row of a matrix to form a vector, <em>i.e.</em>,</p>
<div class="math notranslate nohighlight">
\[v_j = \sum_{i} A_{ij}\]</div>
<p>could be written using the generalized Einstein summation convention as:</p>
<div class="math notranslate nohighlight">
\[v_j = A_{ij}\]</div>
<p>similarly the trace of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> can be written:</p>
<div class="math notranslate nohighlight">
\[Tr\left(\mathbf{A}\right) = A_{ii}.\]</div>
<p>For the purposes of TensorWrapper, generalized Einstein summation convention
allows us to write many tensor operations in a user-friendly manner. For example
the above equations</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Vector-vector inner-product</span>
<span class="n">TensorWrapper</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">;</span>
<span class="n">c</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u</span><span class="p">(</span><span class="s">&quot;i&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">v</span><span class="p">(</span><span class="s">&quot;i&quot;</span><span class="p">);</span>

<span class="c1">// Vector-matrix product</span>
<span class="n">TensorWrapper</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">vM</span><span class="p">;</span>
<span class="n">vM</span><span class="p">(</span><span class="s">&quot;j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">v</span><span class="p">(</span><span class="s">&quot;i&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">M</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">);</span>

<span class="c1">// Matrix-matrix product</span>
<span class="n">TensorWrapper</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">;</span>
<span class="n">C</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="s">&quot;i,k&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">(</span><span class="s">&quot;k,j&quot;</span><span class="p">);</span>

<span class="c1">// Element-wise matrix product</span>
<span class="n">C</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">);</span>

<span class="c1">// Summing over a row of a matrix</span>
<span class="n">v</span><span class="p">(</span><span class="s">&quot;j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">);</span>

<span class="c1">// Trace of a matrix</span>
<span class="n">c</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="s">&quot;i,i&quot;</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Link to this heading"></a></h2>
<p>Traditional Einstein summation convention is usually said to be limited to pairs
of repeated indices because repeating an index three or more times is ambiguous
(<em>e.g.</em>, see
<a class="reference external" href="https://math.stackexchange.com/questions/436515/problem-with-free-index-in-einstein-summation-notation">here</a>).
Consider the equation:</p>
<div class="math notranslate nohighlight">
\[u_{i} = v_{i}B_{ii}.\]</div>
<p>Traditional Einstein summation convention requires that we must sum over pairs
of repeated indices. Since <span class="math notranslate nohighlight">\(i\)</span> appears more than once we must sum it.
There are three possible ways to sum over pairs of <span class="math notranslate nohighlight">\(i\)</span>. For clarity,
we perform a dummy index transformation so that we are summing over <span class="math notranslate nohighlight">\(j\)</span>
instead:</p>
<div class="math notranslate nohighlight">
\[u_{i} = \sum_{j} v_j B_{ji}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[u_{i} = \sum_{j} v_{j}B_{ij}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[u_{i} = \sum_{j}  v_{i} B_{jj}.\]</div>
<p>We however argue that none of these interpretations are in the spirit of
conventional summation notation because changing the value of a dummy index
must be done to all occurrences of the dummy index in order to preserve the
meaning. Even for vectors we can not selectively change dummy indices without
changing the meaning, <em>i.e.</em>,</p>
<div class="math notranslate nohighlight">
\[\sum_{i} u_iv_i \neq \sum_{ij} u_iv_j.\]</div>
<p>The general Einstein summation convention has no ambiguity for three repeated
indices and, consistent with conventional summation conventions, recognizes
<span class="math notranslate nohighlight">\(u_i=v_iB_{ii}\)</span> as the product of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and the diagonal of <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>. In fact,
generalized Einstein summation convention has no ambiguity since
every index is either summed over, or not, based on whether it appears on one or
both sides of an equation respectively. Indices which must have the same values
in each term must be assigned the same letter. Indices which are allowed to
vary independently must be assigned different letters.</p>
<p>That said, in dynamic programming situations it can be hard to ensure indices
are chosen in a manner which adheres to the general Einstein summation
convention. For example, it is not unreasonable to write something like:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">){</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">]</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">build_tensors_from_parameter</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="s">&quot;i,k&quot;</span><span class="p">)</span><span class="o">*</span><span class="n">B</span><span class="p">(</span><span class="s">&quot;k,j&quot;</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">rhs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">l</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">l</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">TensorWrapper</span><span class="w"> </span><span class="n">C</span><span class="p">;</span>
<span class="n">C</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rhs</span><span class="p">;</span>
</pre></div>
</div>
<p>The idea being we have a function which generates terms for an expression, then
the caller of the function assembles those terms into a larger expression before
ultimately assigning it to an indexed tensor. As written the above would
generate an expression which looks something like (note that <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>
in the lambda are temporary variable names):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">C</span><span class="p">(</span><span class="s">&quot;i,j&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="s">&quot;i,k&quot;</span><span class="p">)</span><span class="o">*</span><span class="n">B</span><span class="p">(</span><span class="s">&quot;k,j&quot;</span><span class="p">)</span><span class="o">*</span><span class="w"> </span><span class="n">D</span><span class="p">(</span><span class="s">&quot;i,k&quot;</span><span class="p">)</span><span class="o">*</span><span class="n">E</span><span class="p">(</span><span class="s">&quot;k,j&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>This is an unambiguous expression, with summations inserted it’s equivalent to:</p>
<div class="math notranslate nohighlight">
\[C_{ij} = \sum_{k}\left(A_{ik}B_{kj}D_{ik}E_{kj}\right)\]</div>
<p>which is not the same as:</p>
<div class="math notranslate nohighlight">
\[C_{ij} = \sum_{kl}\left(A_{ik}B_{kj}D_{il}E_{lj}\right).\]</div>
<p>Point being, if the intent of the function calls was to return a matrix in a
factorized form, they needed to choose different contraction indices in each
call. Generally speaking, generalized Einstein summation convention is best
applied to binary operations and not to nested expressions.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="terminology.html" class="btn btn-neutral float-left" title="TensorWrapper Terminology" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="logical_v_physical.html" class="btn btn-neutral float-right" title="Logical Versus Physical: Understanding the Tensor Layout" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, NWChemEx Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>